import streamlit as st
from models import get_embeddings, get_llm_engine
from database import sync_swagger
from graph import create_retrieval_graph

st.set_page_config(page_title="API AI Assistant", layout="wide")

# 1. ì‚¬ì´ë“œë°” - ì„¤ì • ì˜ì—­
# st.sidebar: ì„¤ì • ì°½ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ì„ ë°”ê¾¸ê±°ë‚˜ ìƒˆë¡œìš´ Swagger URLì„ ë„£ì–´ ì§€ì‹ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
with st.sidebar:
    st.header("âš™ï¸ ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •")
    
    # LLM ì„ íƒ UI
    llm_type = st.selectbox("LLM ì—”ì§„", ["Local (Ollama)", "External API"])
    if llm_type == "Local (Ollama)":
        m_name = st.text_input("Ollama ëª¨ë¸ëª…", value="llama3")
        api_url, api_key = None, None
    else:
        api_url = st.text_input("API URL", value="https://api.openai.com/v1")
        m_name = st.text_input("ëª¨ë¸ëª…", value="gpt-4o")
        api_key = st.text_input("API Key", type="password")

    st.divider()
    
    # Swagger ë™ê¸°í™” UI
    sw_url = st.text_input("Swagger JSON URL")
    if st.button("ì§€ì‹ë² ì´ìŠ¤ ë™ê¸°í™”"):
        emb = get_embeddings(api_url, api_key)
        try:
            count = sync_swagger(sw_url, emb)
            st.success(f"{count}ê°œ API ëª…ì„¸ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.error(e)

# 2. ë©”ì¸ ì±„íŒ… ì˜ì—­
st.title("ğŸ›¡ï¸ Self-Correction API Assistant")
st.caption("Swagger ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ AIê°€ íŒë‹¨í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.")

# ëª¨ë¸ ë¡œë“œ (ìºì‹±ì„ í†µí•´ ì†ë„ í–¥ìƒ ê°€ëŠ¥í•˜ë‚˜ ì—¬ê¸°ì„  ì§ê´€ì ìœ¼ë¡œ í‘œí˜„)
embeddings = get_embeddings(api_url, api_key)
llm = get_llm_engine(llm_type, m_name, api_url, api_key)
app = create_retrieval_graph(embeddings, llm)

# st.session_state.messages: ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•©ë‹ˆë‹¤. Streamlitì€ ìƒˆë¡œê³ ì¹¨ì´ ì¦ê¸° ë•Œë¬¸ì— ì´ ë³€ìˆ˜ì— ê¸°ë¡ì„ ìŒ“ì•„ë‘ì–´ì•¼ ëŒ€í™”ê°€ ëŠê¸°ì§€ ì•ŠìŠµë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # st.spinner: AIê°€ 'ìƒê°(ì±„ì  ë° ì¬ê²€ìƒ‰)'í•˜ëŠ” ë™ì•ˆ ì‚¬ìš©ìì—ê²Œ "ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”"ë¼ëŠ” ì‹œê°ì  í”¼ë“œë°±ì„ ì¤ë‹ˆë‹¤.
        with st.spinner("AIê°€ ì§€ì‹ì„ ê²€ì¦í•˜ë©° ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # app.invoke(): ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ê°€ë™ì‹œí‚¤ëŠ” ìŠ¤ìœ„ì¹˜ ì—­í• ì„ í•©ë‹ˆë‹¤.
            result = app.invoke({"question": prompt, "iteration": 0})
            ans = result["generation"]
            st.markdown(ans)
            if result["iteration"] > 1:
                st.info("ğŸ’¡ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•˜ì˜€ìŠµë‹ˆë‹¤.")
                
    st.session_state.messages.append({"role": "assistant", "content": ans})