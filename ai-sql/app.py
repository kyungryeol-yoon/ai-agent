import streamlit as st
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. í˜ì´ì§€ ì„¤ì • ë° ë ˆì´ì•„ì›ƒ
st.set_page_config(page_title="Local SQL/API Agent", layout="wide")
st.title("ğŸ¤– Local SQL & Swagger Assistant")
st.markdown("""
ì´ ì‹œìŠ¤í…œì€ **BGE-M3** ëª¨ë¸ì„ í†µí•´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ , 
**Ollama(Llama3)**ë¥¼ í†µí•´ ë¡œì»¬ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
""")

# 2. ëª¨ë¸ ë° DB ë¡œë“œ (ìºì‹± ì²˜ë¦¬í•˜ì—¬ ì†ë„ ìµœì í™”)
@st.cache_resource
def initialize_system():
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (BGE-M3)
    # ìˆ˜ì¹˜í™” ì›ë¦¬: í…ìŠ¤íŠ¸ë¥¼ 1024ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰)
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # ì˜ˆì‹œ ë°ì´í„° (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” íŒŒì¼ ë¡œë”ë¥¼ í†µí•´ í™•ì¥ ê°€ëŠ¥)
    raw_data = [
        {"content": "SELECT * FROM orders WHERE status = 'PENDING';", "desc": "ëŒ€ê¸° ì¤‘ì¸ ì£¼ë¬¸ì„ ì¡°íšŒí•˜ëŠ” SQLì…ë‹ˆë‹¤."},
        {"content": "POST /api/v1/login", "desc": "ì‚¬ìš©ì ë¡œê·¸ì¸ì„ ìœ„í•œ Swagger API ëª…ì„¸ì…ë‹ˆë‹¤. ID/PWê°€ í•„ìš”í•©ë‹ˆë‹¤."},
        {"content": "CREATE TABLE users (id INT, name TEXT);", "desc": "ì‚¬ìš©ì í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” DDL ë¬¸ì…ë‹ˆë‹¤."}
    ]
    docs = [Document(page_content=f"{d['content']}\nì„¤ëª…: {d['desc']}") for d in raw_data]
    
    # Vector DB êµ¬ì¶• (ë¡œì»¬ ì €ì¥ì†Œ: ./chroma_db)
    vectorstore = Chroma.from_documents(
        documents=docs, 
        embedding=embeddings, 
        persist_directory="./chroma_db"
    )
    
    # LLM ì„¤ì • (Ollama)
    llm = OllamaLLM(model="llama3")
    
    return vectorstore.as_retriever(search_kwargs={"k": 2}), llm

retriever, llm = initialize_system()

# 3. RAG ì²´ì¸ êµ¬ì„± (LangChain)
template = """ë‹¹ì‹ ì€ IT ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë¬¸ë§¥(Context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ëª¨ë¥´ëŠ” ë‚´ìš©ì´ë¼ë©´ ì–µì§€ë¡œ ë§Œë“¤ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

[Context]
{context}

[Question]
{question}

ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 4. ì±„íŒ… UI êµ¬í˜„
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì±„íŒ… ì…ë ¥ì°½
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë¡œê·¸ì¸ API ì •ë³´ ì•Œë ¤ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ë¡œì»¬ ì§€ì‹ ë² ì´ìŠ¤(Vector DB) ê²€ìƒ‰ ì¤‘..."):
            response = rag_chain.invoke(user_input)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})